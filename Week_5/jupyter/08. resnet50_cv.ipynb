{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40efe3f8-1ece-4c2a-98d7-35ed7a59a6f3",
   "metadata": {},
   "source": [
    "# Model 1:\n",
    "\n",
    "- model: resnet50\n",
    "- batch_size: 16\n",
    "- lr: 1e-5\n",
    "- epochs: 5\n",
    "- criterion: LabelSmoothingLoss\n",
    "- optimizer: MadGrad\n",
    "- transform: Augmentation6\n",
    "- cross validation: Stratified 5-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bae9be-5382-4cc3-bf2c-97b7cc1b9c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "This notebook use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Data preprocessing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "torch.manual_seed(0)\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "# device setting\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'This notebook use {device}')\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d19aac-21ee-449b-9b32-ae978e125ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 사용자 정의\n",
    "class path:\n",
    "    data = '/opt/ml/input/original_data'\n",
    "    train = f'{data}/train'\n",
    "    train_img = f'{train}/images'\n",
    "    train_df = f'{train}/train.csv'\n",
    "    test = f'{data}/eval'\n",
    "    test_img = f'{test}/images'\n",
    "    test_df = f'{test}/info.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe31d1b-15b7-4277-93d6-bc21242ae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0e776-dbf7-4cdf-bb53-a8cc376843f6",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a73509-7bdf-4d81-92dd-0b8c6d425f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.iloc[idx]\n",
    "        target = data.target\n",
    "        image = Image.open(data.path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba44983-3706-4228-b43b-770b662cd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3a0094-0eff-4931-9800-fcf203d3a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.548, 0.504, 0.479], std=[0.237, 0.247, 0.246]),\n",
    "    AddGaussianNoise(0., 1.),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5f17a0-d0b5-4bbb-9e15-85704882e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.548, 0.504, 0.479], std=[0.237, 0.247, 0.246]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6a1dc-4348-4af5-be24-9c104f40b569",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77dec1e9-215e-4a0e-b584-671aabac1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.optim.optimizer import _params_t\n",
    "else:\n",
    "    _params_t = Any\n",
    "    \n",
    "class MADGRAD(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic \n",
    "    Optimization.\n",
    "    .. _MADGRAD: https://arxiv.org/abs/2101.11075\n",
    "    MADGRAD is a general purpose optimizer that can be used in place of SGD or\n",
    "    Adam may converge faster and generalize better. Currently GPU-only.\n",
    "    Typically, the same learning rate schedule that is used for SGD or Adam may\n",
    "    be used. The overall learning rate is not comparable to either method and\n",
    "    should be determined by a hyper-parameter sweep.\n",
    "    MADGRAD requires less weight decay than other methods, often as little as\n",
    "    zero. Momentum values used for SGD or Adam's beta1 should work here also.\n",
    "    On sparse problems both weight_decay and momentum should be set to 0.\n",
    "    Arguments:\n",
    "        params (iterable): \n",
    "            Iterable of parameters to optimize or dicts defining parameter groups.\n",
    "        lr (float): \n",
    "            Learning rate (default: 1e-2).\n",
    "        momentum (float): \n",
    "            Momentum value in  the range [0,1) (default: 0.9).\n",
    "        weight_decay (float): \n",
    "            Weight decay, i.e. a L2 penalty (default: 0).\n",
    "        eps (float): \n",
    "            Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-6).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, params: _params_t, lr: float = 1e-2, momentum: float = 0.9, weight_decay: float = 0, eps: float = 1e-6,\n",
    "    ):\n",
    "        if momentum < 0 or momentum >= 1:\n",
    "            raise ValueError(f\"Momentum {momentum} must be in the range [0,1]\")\n",
    "        if lr <= 0:\n",
    "            raise ValueError(f\"Learning rate {lr} must be positive\")\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n",
    "        if eps < 0:\n",
    "            raise ValueError(f\"Eps must be non-negative\")\n",
    "\n",
    "        defaults = dict(lr=lr, eps=eps, momentum=momentum, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # step counter must be stored in state to ensure correct behavior under\n",
    "        # optimizer sharding\n",
    "        if 'k' not in self.state:\n",
    "            self.state['k'] = torch.tensor([0], dtype=torch.long)\n",
    "        k = self.state['k'].item()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            eps = group[\"eps\"]\n",
    "            lr = group[\"lr\"] + eps\n",
    "            decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            ck = 1 - momentum\n",
    "            lamb = lr * math.pow(k + 1, 0.5)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                if \"grad_sum_sq\" not in state:\n",
    "                    state[\"grad_sum_sq\"] = torch.zeros_like(p.data).detach()\n",
    "                    state[\"s\"] = torch.zeros_like(p.data).detach()\n",
    "                    if momentum != 0:\n",
    "                        state[\"x0\"] = torch.clone(p.data).detach()\n",
    "\n",
    "                if momentum != 0.0 and grad.is_sparse:\n",
    "                    raise RuntimeError(\"momentum != 0 is not compatible with sparse gradients\")\n",
    "\n",
    "                grad_sum_sq = state[\"grad_sum_sq\"]\n",
    "                s = state[\"s\"]\n",
    "\n",
    "                # Apply weight decay\n",
    "                if decay != 0:\n",
    "                    if grad.is_sparse:\n",
    "                        raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n",
    "\n",
    "                    grad.add_(p.data, alpha=decay)\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    grad = grad.coalesce()\n",
    "                    grad_val = grad._values()\n",
    "\n",
    "                    p_masked = p.sparse_mask(grad)\n",
    "                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n",
    "                    s_masked = s.sparse_mask(grad)\n",
    "\n",
    "                    # Compute x_0 from other known quantities\n",
    "                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 / 3).add_(eps)\n",
    "                    x0_masked_vals = p_masked._values().addcdiv(s_masked._values(), rms_masked_vals, value=1)\n",
    "\n",
    "                    # Dense + sparse op\n",
    "                    grad_sq = grad * grad\n",
    "                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n",
    "                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n",
    "\n",
    "                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 / 3).add_(eps)\n",
    "\n",
    "                    s.add_(grad, alpha=lamb)\n",
    "                    s_masked._values().add_(grad_val, alpha=lamb)\n",
    "\n",
    "                    # update masked copy of p\n",
    "                    p_kp1_masked_vals = x0_masked_vals.addcdiv(s_masked._values(), rms_masked_vals, value=-1)\n",
    "                    # Copy updated masked p to dense p using an add operation\n",
    "                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n",
    "                    p.data.add_(p_masked, alpha=-1)\n",
    "                else:\n",
    "                    if momentum == 0:\n",
    "                        # Compute x_0 from other known quantities\n",
    "                        rms = grad_sum_sq.pow(1 / 3).add_(eps)\n",
    "                        x0 = p.data.addcdiv(s, rms, value=1)\n",
    "                    else:\n",
    "                        x0 = state[\"x0\"]\n",
    "\n",
    "                    # Accumulate second moments\n",
    "                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n",
    "                    rms = grad_sum_sq.pow(1 / 3).add_(eps)\n",
    "\n",
    "                    # Update s\n",
    "                    s.data.add_(grad, alpha=lamb)\n",
    "\n",
    "                    # Step\n",
    "                    if momentum == 0:\n",
    "                        p.data.copy_(x0.addcdiv(s, rms, value=-1))\n",
    "                    else:\n",
    "                        z = x0.addcdiv(s, rms, value=-1)\n",
    "\n",
    "                        # p is a moving average of z\n",
    "                        p.data.mul_(1 - ck).add_(z, alpha=ck)\n",
    "\n",
    "\n",
    "        self.state['k'] += 1\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e7a38e-3d38-49c7-a4d6-807526122482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://github.com/pytorch/pytorch/issues/7455\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb1d9a52-924c-471d-bcaf-a59c9ea9b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "n_features = model.fc.in_features\n",
    "model.fc = nn.Linear(n_features, 18)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = MADGRAD(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "criterion = LabelSmoothingLoss(classes=18, smoothing=0.1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c371557-0f32-4e6a-a196-78721efe6da4",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff72b35-edeb-4e57-acb6-fd59b88bbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model, valid_dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true, y_pred = [], []\n",
    "        for image, label in tqdm(valid_dataset):\n",
    "            X = image.float().to(device)\n",
    "            y = label.item()\n",
    "            _, pred = torch.max(model(X), 1)\n",
    "            pred = pred.item()\n",
    "            y_true.append(y)\n",
    "            y_pred.append(pred)\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "    model.train()\n",
    "    return f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed26a481-6c94-4b01-9450-c5eb658789c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "\n",
    "def train_model(train, test, model, criterion, optimizer, scheduler, cv, print_every=1):\n",
    "    print(f\"============ Training Starts! ============\")\n",
    "    global best_f1\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_sum = 0\n",
    "        for images, label in tqdm(train):\n",
    "            X = images.float().to(device)\n",
    "            y = label.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += loss\n",
    "            \n",
    "        # lr 감소\n",
    "        scheduler.step()\n",
    "            \n",
    "        # 평가지표 출력\n",
    "        if ((epoch % print_every) == 0) or (epoch == (EPOCHS - 1)):\n",
    "            loss_avg = loss_sum / len(train)\n",
    "            f1, accuracy = test_eval(model, test)\n",
    "            print(f\">> epoch:[{epoch + 1}/{EPOCHS}] cost: {loss_avg:5.3f} test_accuracy: {accuracy:5.3f} test_f1_score: {f1:5.3f}\")\n",
    "            if f1 > best_f1:\n",
    "                torch.save(model.state_dict(), f'./model/resnet_50/cv_{cv}_epoch_{epoch}_cost_{loss_avg:.2f}_accr_{accuracy:.2f}_f1_{f1:.2f}.pt')\n",
    "                best_f1 = f1\n",
    "            \n",
    "    print(f\"============ Training Done! ============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41d9167b-2d6d-43af-b121-d9c9617fad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(df, model, criterion, optimizer, scheduler, k_folds=5):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    for n_iter, (train_idx, valid_idx) in enumerate(skf.split(df, df.target), start=1):\n",
    "        print(f'>> Cross Validation {n_iter} Starts!')\n",
    "        train, valid = df.loc[train_idx], df.loc[valid_idx]\n",
    "        train_dataset, valid_dataset = MaskDataset(train), MaskDataset(valid)\n",
    "        \n",
    "        # augmentation 설정\n",
    "        train_dataset.set_transform(train_transforms)\n",
    "        valid_dataset.set_transform(valid_transforms)\n",
    "        \n",
    "        # DataLoader 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, shuffle=False)\n",
    "        \n",
    "        train_model(train_loader, valid_loader, model, criterion, optimizer, scheduler, n_iter)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b698c8fc-7299-4284-a588-4cce8afe209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{path.train}/train_modified.csv')[['path', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c325cfa-fec8-461e-89e1-15f49895aa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cross Validation 1 Starts!\n",
      "============ Training Starts! ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:08<00:00,  7.38it/s]\n",
      "100%|██████████| 3780/3780 [02:04<00:00, 30.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[1/5] cost: 1.100 test_accuracy: 0.762 test_f1_score: 0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:21<00:00,  6.69it/s]\n",
      "100%|██████████| 3780/3780 [01:52<00:00, 33.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[2/5] cost: 1.025 test_accuracy: 0.763 test_f1_score: 0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:18<00:00,  6.81it/s]\n",
      "100%|██████████| 3780/3780 [01:56<00:00, 32.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[3/5] cost: 0.973 test_accuracy: 0.743 test_f1_score: 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:23<00:00,  6.58it/s]\n",
      "100%|██████████| 3780/3780 [01:54<00:00, 32.93it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[4/5] cost: 0.896 test_accuracy: 0.765 test_f1_score: 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:15<00:00,  6.97it/s]\n",
      "100%|██████████| 3780/3780 [02:04<00:00, 30.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[5/5] cost: 0.873 test_accuracy: 0.767 test_f1_score: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Training Done! ============\n",
      "\n",
      ">> Cross Validation 2 Starts!\n",
      "============ Training Starts! ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:02<00:00,  7.74it/s]\n",
      "100%|██████████| 3780/3780 [02:10<00:00, 28.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[1/5] cost: 0.915 test_accuracy: 0.874 test_f1_score: 0.755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [01:50<00:00,  8.53it/s]\n",
      "100%|██████████| 3780/3780 [02:09<00:00, 29.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[2/5] cost: 0.883 test_accuracy: 0.871 test_f1_score: 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:04<00:00,  7.57it/s]\n",
      "100%|██████████| 3780/3780 [02:07<00:00, 29.75it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[3/5] cost: 0.859 test_accuracy: 0.863 test_f1_score: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:21<00:00,  6.68it/s]\n",
      "100%|██████████| 3780/3780 [01:57<00:00, 32.31it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[4/5] cost: 0.843 test_accuracy: 0.859 test_f1_score: 0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:22<00:00,  6.62it/s]\n",
      "100%|██████████| 3780/3780 [01:56<00:00, 32.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[5/5] cost: 0.826 test_accuracy: 0.852 test_f1_score: 0.766\n",
      "============ Training Done! ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Cross Validation 3 Starts!\n",
      "============ Training Starts! ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:17<00:00,  6.89it/s]\n",
      "100%|██████████| 3780/3780 [01:56<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[1/5] cost: 0.828 test_accuracy: 0.907 test_f1_score: 0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:06<00:00,  7.48it/s]\n",
      "100%|██████████| 3780/3780 [02:09<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[2/5] cost: 0.809 test_accuracy: 0.891 test_f1_score: 0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [01:54<00:00,  8.29it/s]\n",
      "100%|██████████| 3780/3780 [02:10<00:00, 28.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[3/5] cost: 0.797 test_accuracy: 0.892 test_f1_score: 0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:03<00:00,  7.67it/s]\n",
      "100%|██████████| 3780/3780 [02:04<00:00, 30.27it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[4/5] cost: 0.785 test_accuracy: 0.884 test_f1_score: 0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:22<00:00,  6.62it/s]\n",
      "100%|██████████| 3780/3780 [02:03<00:00, 30.56it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[5/5] cost: 0.770 test_accuracy: 0.892 test_f1_score: 0.789\n",
      "============ Training Done! ============\n",
      "\n",
      ">> Cross Validation 4 Starts!\n",
      "============ Training Starts! ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:23<00:00,  6.56it/s]\n",
      "100%|██████████| 3780/3780 [01:56<00:00, 32.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[1/5] cost: 0.782 test_accuracy: 0.910 test_f1_score: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:20<00:00,  6.71it/s]\n",
      "100%|██████████| 3780/3780 [01:57<00:00, 32.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[2/5] cost: 0.770 test_accuracy: 0.902 test_f1_score: 0.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:12<00:00,  7.15it/s]\n",
      "100%|██████████| 3780/3780 [02:07<00:00, 29.60it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[3/5] cost: 0.764 test_accuracy: 0.901 test_f1_score: 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [01:59<00:00,  7.91it/s]\n",
      "100%|██████████| 3780/3780 [02:03<00:00, 30.73it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[4/5] cost: 0.753 test_accuracy: 0.893 test_f1_score: 0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [01:51<00:00,  8.47it/s]\n",
      "100%|██████████| 3780/3780 [01:08<00:00, 55.06it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[5/5] cost: 0.742 test_accuracy: 0.897 test_f1_score: 0.812\n",
      "============ Training Done! ============\n",
      "\n",
      ">> Cross Validation 5 Starts!\n",
      "============ Training Starts! ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [01:48<00:00,  8.67it/s]\n",
      "100%|██████████| 3780/3780 [01:07<00:00, 55.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[1/5] cost: 0.750 test_accuracy: 0.910 test_f1_score: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [01:48<00:00,  8.72it/s]\n",
      "100%|██████████| 3780/3780 [01:09<00:00, 54.53it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[2/5] cost: 0.742 test_accuracy: 0.918 test_f1_score: 0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [01:51<00:00,  8.51it/s]\n",
      "100%|██████████| 3780/3780 [01:48<00:00, 34.83it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[3/5] cost: 0.735 test_accuracy: 0.910 test_f1_score: 0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:24<00:00,  6.56it/s]\n",
      "100%|██████████| 3780/3780 [01:50<00:00, 34.34it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[4/5] cost: 0.726 test_accuracy: 0.919 test_f1_score: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [02:22<00:00,  6.62it/s]\n",
      "100%|██████████| 3780/3780 [01:57<00:00, 32.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch:[5/5] cost: 0.720 test_accuracy: 0.909 test_f1_score: 0.824\n",
      "============ Training Done! ============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validation(df, model, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ca10a-28a2-4d0c-baf4-0fc99412ec50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
