{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from pytorch_pretrained_vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793b7cf4-fd7a-485d-ab96-1ee4ea92f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a8dffd-109b-4756-a4ca-679e6f91dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4ee1fc-c5fd-4009-b590-d1bf3a92ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "SEED = 2021\n",
    "# random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    train_dir = '/opt/ml/input/data/train'\n",
    "    img_dir = f'{train_dir}/images'\n",
    "    df_path = f'{train_dir}/train_with_label.csv'\n",
    "    \n",
    "    model_save_path = '/opt/ml/code/model'\n",
    "    sumission_save_path = '/opt/ml/code/sumission'\n",
    "    \n",
    "    test_dir = '/opt/ml/input/data/eval'\n",
    "    submission_dir = f'''{sumission_save_path}/\n",
    "        {datetime.datetime.today().astimezone(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d\")}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cf1878-b551-4827-9311-e46640ac66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(cfg.submission_dir):\n",
    "    os.mkdir(cfg.submission_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdfb7e07-276d-44c6-9c52-90cc796117a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is using!\n"
     ]
    }
   ],
   "source": [
    "## HYPER PARAMETER 정의\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 0.001\n",
    "CLASS_NUM = 18\n",
    "IMAGE_SIZE = 197\n",
    "PATIENCE = 7\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is using!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72120758-fa63-4603-9aa7-575558925147",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Pretrained Model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a43250-f95e-4f5e-b2da-613bee424c1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized positional embeddings from torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])\n",
      "Loaded pretrained weights.\n",
      "네트워크 필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 (예측 class type 개수) 21843\n",
      "ViT(\n",
      "  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (positional_embedding): PositionalEmbedding1D()\n",
      "  (transformer): Transformer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc): Linear(in_features=768, out_features=21843, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vit = ViT('B_16', image_size=IMAGE_SIZE, pretrained=True)\n",
    "print('네트워크 필요 입력 채널 개수', vit.patch_embedding.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수 (예측 class type 개수)', vit.fc.weight.shape[0])\n",
    "print(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df7e3ef-516f-4c21-9c12-1a8238a87832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 출력 채널 개수 (예측 class type 개수) 18\n"
     ]
    }
   ],
   "source": [
    "# target model의 출력 크기를 변경하여 줍니다.\n",
    "vit.fc = torch.nn.Linear(in_features=768, out_features=CLASS_NUM, bias=True)\n",
    "\n",
    "# 새롭게 넣은 네트워크 가중치를 xavier uniform으로 초기화\n",
    "torch.nn.init.xavier_uniform_(vit.fc.weight)\n",
    "stdv = 1.0/np.sqrt(768)\n",
    "vit.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "print('네트워크 출력 채널 개수 (예측 class type 개수)', vit.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ec19e-f2d8-4fdd-821d-d365fc8a2a01",
   "metadata": {},
   "source": [
    "for param in efficientnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff51c3-0cb1-4d95-a58a-9d92f64d2498",
   "metadata": {},
   "source": [
    "for param in efficientnet._fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be63323-9202-4f52-9ad2-4bee7abb8922",
   "metadata": {},
   "source": [
    "### requires_grad = False 적용 됐는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c1180-4aca-4342-a3fd-9e7611c06dcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "list(efficientnet._conv_stem.parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3761434-ba5e-4844-991b-aeaa41c50288",
   "metadata": {},
   "source": [
    "list(efficientnet._fc.parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Train Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.img_paths = df['image_path']\n",
    "        self.transform = transform\n",
    "        self.y = df['target']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths.iloc[idx])\n",
    "        label = self.y.iloc[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8aff6b-4bf3-4be4-a0dd-03a101a5e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(cfg.train_dir, 'train_with_label.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae48762-b59b-41db-ab6d-eb0985c60e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mask</th>\n",
       "      <th>path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_band</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Incorrect</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>&gt;= 30 and &lt; 60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wear</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>&gt;= 30 and &lt; 60</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wear</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>&gt;= 30 and &lt; 60</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Wear</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>&gt;= 30 and &lt; 60</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wear</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>&gt;= 30 and &lt; 60</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mask                    path  \\\n",
       "0  Incorrect  000001_female_Asian_45   \n",
       "1       Wear  000001_female_Asian_45   \n",
       "2       Wear  000001_female_Asian_45   \n",
       "3   Not Wear  000001_female_Asian_45   \n",
       "4       Wear  000001_female_Asian_45   \n",
       "\n",
       "                                          image_path  age  gender  \\\n",
       "0  /opt/ml/input/data/train/images/000001_female_...   45  female   \n",
       "1  /opt/ml/input/data/train/images/000001_female_...   45  female   \n",
       "2  /opt/ml/input/data/train/images/000001_female_...   45  female   \n",
       "3  /opt/ml/input/data/train/images/000001_female_...   45  female   \n",
       "4  /opt/ml/input/data/train/images/000001_female_...   45  female   \n",
       "\n",
       "         age_band  target  \n",
       "0  >= 30 and < 60      10  \n",
       "1  >= 30 and < 60       4  \n",
       "2  >= 30 and < 60       4  \n",
       "3  >= 30 and < 60      16  \n",
       "4  >= 30 and < 60       4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f935da08-8895-4262-aaa8-129aafa61fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13230, 7), (5670, 7))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=0.3, stratify=df['target'], shuffle=True, random_state=2021)\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050172f0-02a6-4c9d-bca4-ef04e64ee270",
   "metadata": {},
   "source": [
    "# train_df 이미지의 pixel 값의 mean & std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a44b53-d5f9-481d-a47c-7360b0a958b5",
   "metadata": {},
   "source": [
    "images_info = {'height': [], 'width': [], 'means': [], 'stds': []}\n",
    "for i, image_path in enumerate(tqdm(train_df.image_path)):\n",
    "    img = np.array(Image.open(image_path))\n",
    "    h, w, _ = img.shape\n",
    "    images_info['height'].append(h)\n",
    "    images_info['width'].append(w)\n",
    "    images_info['means'].append(img.mean(axis=(0, 1)))\n",
    "    images_info['stds'].append(img.std(axis=(0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468e83a-97c2-413d-babd-00b37d13a44d",
   "metadata": {},
   "source": [
    "print('Train Data')\n",
    "\n",
    "print(f'Minimum height for dataset is {np.min(images_info[\"height\"])}')\n",
    "print(f'Maximum height for dataset is {np.max(images_info[\"height\"])}')\n",
    "print(f'Average height for dataset is {int(np.mean(images_info[\"height\"]))}')\n",
    "print(f'Minimum width for dataset is {np.min(images_info[\"width\"])}')\n",
    "print(f'Maximum width for dataset is {np.max(images_info[\"width\"])}')\n",
    "print(f'Average width for dataset is {int(np.mean(images_info[\"width\"]))}')\n",
    "\n",
    "print(f'RGB Mean: {np.mean(images_info[\"means\"], axis=0) / 255.}')\n",
    "print(f'RGB Standard Deviation: {np.mean(images_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e7cd653-b5e1-4aa9-9fe1-e56f1ea7d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list = [0.56074416, 0.52454219, 0.50188343]\n",
    "std_list = [0.23304677, 0.24291714, 0.24565602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0928239c-354a-4412-9e89-a9efc08ae27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomChoice([transforms.ColorJitter(brightness=(0.2, 3)),\n",
    "                             transforms.ColorJitter(contrast=(0.2, 3)),\n",
    "                             transforms.ColorJitter(saturation=(0.2, 3)),\n",
    "                             transforms.ColorJitter(hue=(-0.3, 0.3)),\n",
    "                            ]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_list, \n",
    "                         std_list)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e92a8f3-6d6d-418e-a295-c43adcc823bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13230, 5670)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_df, transform)\n",
    "valid_dataset = MyDataset(valid_df, transform=transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_list, \n",
    "                         std_list)\n",
    "]))\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de510e9-e3b5-4131-963c-c720318c2fe6",
   "metadata": {},
   "source": [
    "## 3. Train DataLoader 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "420c29dc-15aa-4aa5-95cb-a99926918517",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cedd39-f97e-49c1-8279-5725d0bf8e0a",
   "metadata": {},
   "source": [
    "## 4. Criterion & Optimizer & lr_scheduler & EarlyStopping 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91324d6-68eb-482e-90da-2b5f955dece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_f1_score_max = -np.Inf\n",
    "\n",
    "    def __call__(self, val_f1_score, model):\n",
    "\n",
    "        score = val_f1_score\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1_score, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_f1_score, model):\n",
    "        '''Saves model when validation f1_score increase.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation f1_score increased ({self.val_f1_score_max:.6f} --> {val_f1_score:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), f'{os.path.join(cfg.model_save_path, \"VIT\")}/checkpoint.pt')\n",
    "        self.val_f1_score_max = val_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1481076-e8eb-4641-b55d-221402e7a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit.to(device)\n",
    "\n",
    "# weight = torch.tensor(len(train_df) // train_df.target.value_counts()).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "earlystop = EarlyStopping(patience=PATIENCE, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecfc5b-203c-4f84-8323-e93e1f62e01f",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "babb5e99-8e4f-4541-b374-c8e3478ea984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629887736.1938424"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb416fb1-87f0-4545-8299-5cc9b36569a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | loss : 0.7767 | F1_score : 2.8308: 100%|██████████| 67/67 [01:40<00:00,  1.50s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0127, Accuracy : 0.2360, F1_score : 0.0431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | loss : 0.3391 | F1_score : 0.7174: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (-inf --> 0.026092).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0129, Accuracy : 0.2190, F1_score : 0.0261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | loss : 0.7013 | F1_score : 5.6785: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0114, Accuracy : 0.3158, F1_score : 0.0880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | loss : 0.2439 | F1_score : 5.1207: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.026092 --> 0.183120).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0093, Accuracy : 0.3969, F1_score : 0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | loss : 0.5018 | F1_score : 19.2084: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0079, Accuracy : 0.5084, F1_score : 0.2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | loss : 0.2258 | F1_score : 9.7023: 100%|██████████| 29/29 [00:16<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.183120 --> 0.346454).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0085, Accuracy : 0.4918, F1_score : 0.3465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | loss : 0.3489 | F1_score : 30.7605: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0056, Accuracy : 0.6468, F1_score : 0.4668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | loss : 0.0935 | F1_score : 16.3250: 100%|██████████| 29/29 [00:16<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.346454 --> 0.583731).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0035, Accuracy : 0.7703, F1_score : 0.5837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | loss : 0.2483 | F1_score : 37.9625: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0040, Accuracy : 0.7410, F1_score : 0.5753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | loss : 0.0802 | F1_score : 18.1374: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.583731 --> 0.645787).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0030, Accuracy : 0.8031, F1_score : 0.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | loss : 0.2158 | F1_score : 41.3954: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0035, Accuracy : 0.7769, F1_score : 0.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | loss : 0.0718 | F1_score : 19.1551: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.645787 --> 0.684926).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0027, Accuracy : 0.8179, F1_score : 0.6849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | loss : 0.2012 | F1_score : 42.7799: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0034, Accuracy : 0.7908, F1_score : 0.6476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | loss : 0.0635 | F1_score : 20.3314: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.684926 --> 0.719222).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0026, Accuracy : 0.8428, F1_score : 0.7192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | loss : 0.1274 | F1_score : 49.2639: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0022, Accuracy : 0.8660, F1_score : 0.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | loss : 0.0480 | F1_score : 21.2494: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.719222 --> 0.758460).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0019, Accuracy : 0.8770, F1_score : 0.7585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | loss : 0.1023 | F1_score : 51.3632: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0018, Accuracy : 0.8872, F1_score : 0.7771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | loss : 0.0464 | F1_score : 21.4790: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.758460 --> 0.767736).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0017, Accuracy : 0.8799, F1_score : 0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | loss : 0.0969 | F1_score : 51.9431: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0018, Accuracy : 0.8918, F1_score : 0.7838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | loss : 0.0429 | F1_score : 21.8640: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.767736 --> 0.781434).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0016, Accuracy : 0.8909, F1_score : 0.7814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | loss : 0.0882 | F1_score : 52.9698: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0014, Accuracy : 0.9049, F1_score : 0.8029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | loss : 0.0409 | F1_score : 22.4666: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.781434 --> 0.805703).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0015, Accuracy : 0.8976, F1_score : 0.8057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | loss : 0.0824 | F1_score : 54.1213: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0013, Accuracy : 0.9132, F1_score : 0.8212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | loss : 0.0397 | F1_score : 22.1648: 100%|██████████| 29/29 [00:16<00:00,  1.78it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "Validation Loss : 0.0016, Accuracy : 0.8945, F1_score : 0.7908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | loss : 0.0728 | F1_score : 55.9167: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0011, Accuracy : 0.9239, F1_score : 0.8482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | loss : 0.0393 | F1_score : 22.4690: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n",
      "Validation Loss : 0.0015, Accuracy : 0.9005, F1_score : 0.8033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | loss : 0.0700 | F1_score : 55.2013: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0012, Accuracy : 0.9248, F1_score : 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | loss : 0.0389 | F1_score : 22.4837: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n",
      "Validation Loss : 0.0015, Accuracy : 0.8999, F1_score : 0.7986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | loss : 0.0616 | F1_score : 56.8848: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0010, Accuracy : 0.9359, F1_score : 0.8607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | loss : 0.0366 | F1_score : 22.8658: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.805703 --> 0.822811).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0013, Accuracy : 0.9119, F1_score : 0.8228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | loss : 0.0579 | F1_score : 56.9807: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9391, F1_score : 0.8638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | loss : 0.0360 | F1_score : 23.0272: 100%|██████████| 29/29 [00:16<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.822811 --> 0.825254).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0014, Accuracy : 0.9115, F1_score : 0.8253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | loss : 0.0595 | F1_score : 56.6198: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0010, Accuracy : 0.9360, F1_score : 0.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | loss : 0.0355 | F1_score : 22.7593: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "Validation Loss : 0.0014, Accuracy : 0.9153, F1_score : 0.8142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | loss : 0.0569 | F1_score : 56.8246: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0010, Accuracy : 0.9395, F1_score : 0.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | loss : 0.0352 | F1_score : 22.7572: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9155, F1_score : 0.8159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | loss : 0.0552 | F1_score : 57.9466: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0010, Accuracy : 0.9421, F1_score : 0.8754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | loss : 0.0350 | F1_score : 23.1034: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n",
      "Validation Loss : 0.0014, Accuracy : 0.9145, F1_score : 0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | loss : 0.0558 | F1_score : 57.2259: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9399, F1_score : 0.8690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | loss : 0.0353 | F1_score : 22.8915: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9144, F1_score : 0.8202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | loss : 0.0553 | F1_score : 57.9526: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9430, F1_score : 0.8749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | loss : 0.0349 | F1_score : 23.0655: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9138, F1_score : 0.8186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | loss : 0.0530 | F1_score : 58.0409: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0008, Accuracy : 0.9462, F1_score : 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | loss : 0.0349 | F1_score : 23.1206: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.825254 --> 0.827780).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0013, Accuracy : 0.9163, F1_score : 0.8278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | loss : 0.0539 | F1_score : 57.9047: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9445, F1_score : 0.8775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | loss : 0.0345 | F1_score : 23.1281: 100%|██████████| 29/29 [00:16<00:00,  1.81it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "Validation Loss : 0.0014, Accuracy : 0.9157, F1_score : 0.8259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | loss : 0.0528 | F1_score : 57.9133: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9454, F1_score : 0.8763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | loss : 0.0348 | F1_score : 23.0346: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9160, F1_score : 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | loss : 0.0515 | F1_score : 58.4972: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0008, Accuracy : 0.9484, F1_score : 0.8880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | loss : 0.0347 | F1_score : 23.4399: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.827780 --> 0.837429).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0013, Accuracy : 0.9153, F1_score : 0.8374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | loss : 0.0535 | F1_score : 58.0098: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9448, F1_score : 0.8791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | loss : 0.0348 | F1_score : 22.9682: 100%|██████████| 29/29 [00:16<00:00,  1.77it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9150, F1_score : 0.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | loss : 0.0521 | F1_score : 56.9504: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0008, Accuracy : 0.9432, F1_score : 0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | loss : 0.0349 | F1_score : 22.8984: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9148, F1_score : 0.8168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | loss : 0.0533 | F1_score : 57.6624: 100%|██████████| 67/67 [01:40<00:00,  1.50s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9432, F1_score : 0.8741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | loss : 0.0348 | F1_score : 23.1814: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9145, F1_score : 0.8274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | loss : 0.0508 | F1_score : 58.3614: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9479, F1_score : 0.8835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | loss : 0.0349 | F1_score : 23.1386: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9154, F1_score : 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | loss : 0.0512 | F1_score : 58.0607: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0008, Accuracy : 0.9462, F1_score : 0.8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | loss : 0.0349 | F1_score : 23.4291: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9154, F1_score : 0.8366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | loss : 0.0518 | F1_score : 57.7706: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9457, F1_score : 0.8748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | loss : 0.0350 | F1_score : 23.3654: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1_score increased (0.837429 --> 0.839856).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.0013, Accuracy : 0.9161, F1_score : 0.8399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | loss : 0.0536 | F1_score : 57.6134: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9456, F1_score : 0.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | loss : 0.0349 | F1_score : 23.0147: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9159, F1_score : 0.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | loss : 0.0517 | F1_score : 57.7896: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9456, F1_score : 0.8762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | loss : 0.0345 | F1_score : 23.3897: 100%|██████████| 29/29 [00:16<00:00,  1.81it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n",
      "Validation Loss : 0.0014, Accuracy : 0.9124, F1_score : 0.8318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | loss : 0.0531 | F1_score : 57.2957: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9435, F1_score : 0.8695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | loss : 0.0347 | F1_score : 23.0689: 100%|██████████| 29/29 [00:16<00:00,  1.80it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9154, F1_score : 0.8256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | loss : 0.0520 | F1_score : 57.7607: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9446, F1_score : 0.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | loss : 0.0346 | F1_score : 23.2507: 100%|██████████| 29/29 [00:16<00:00,  1.81it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9148, F1_score : 0.8289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | loss : 0.0514 | F1_score : 58.5112: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0008, Accuracy : 0.9452, F1_score : 0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | loss : 0.0348 | F1_score : 23.1342: 100%|██████████| 29/29 [00:16<00:00,  1.81it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9148, F1_score : 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | loss : 0.0537 | F1_score : 57.9688: 100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009, Accuracy : 0.9442, F1_score : 0.8782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | loss : 0.0347 | F1_score : 23.0367: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 6 out of 7\n",
      "Validation Loss : 0.0013, Accuracy : 0.9145, F1_score : 0.8246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | loss : 0.0518 | F1_score : 58.1777: 100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0008, Accuracy : 0.9456, F1_score : 0.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | loss : 0.0345 | F1_score : 23.0877: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 7 out of 7\n",
      "Validation Loss : 0.0014, Accuracy : 0.9142, F1_score : 0.8267\n",
      "Early stopping\n",
      "Best Validation F1_score: 0.8399\n",
      "학습 종료!\n",
      "CPU times: user 38min 12s, sys: 33min 27s, total: 1h 11min 40s\n",
      "Wall time: 1h 14min 14s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_acc_history = []\n",
    "train_loss_history = []\n",
    "val_acc_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train phase\n",
    "    model.train()\n",
    "    \n",
    "    train_running_loss = 0\n",
    "    train_running_acc = 0\n",
    "    train_running_epoch_f1 = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader)\n",
    "    for X_batch, y_batch in pbar:\n",
    "        pbar.set_description(f'Epoch {epoch}/{EPOCHS} | loss : {train_running_loss:.4f} | F1_score : {train_running_epoch_f1:.4f}')\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).type(torch.cuda.LongTensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model.forward(X_batch)\n",
    "        _, preds = torch.max(y_pred, 1)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss += loss.item() / len(y_batch)\n",
    "        train_running_acc += torch.sum(preds == y_batch.data) / len(y_batch)\n",
    "        train_running_epoch_f1 += f1_score(preds.cpu().numpy(), \n",
    "                                           y_batch.cpu().numpy(), average='macro')\n",
    "        \n",
    "    exp_lr_scheduler.step()\n",
    "    \n",
    "    train_epoch_loss = train_running_loss / len(train_loader)\n",
    "    train_epoch_acc = train_running_acc / len(train_loader)\n",
    "    train_epoch_f1 = train_running_epoch_f1 / len(train_loader)\n",
    "    \n",
    "    train_acc_history.append(train_epoch_acc)\n",
    "    train_loss_history.append(train_epoch_loss)\n",
    "    \n",
    "    print(f'Train Loss : {train_epoch_loss:.4f}, Accuracy : {train_epoch_acc:.4f}, F1_score : {train_epoch_f1:.4f}')\n",
    "    \n",
    "    # Validation pahse\n",
    "    model.eval()\n",
    "    \n",
    "    valid_running_loss = 0\n",
    "    valid_running_acc = 0\n",
    "    valid_running_epoch_f1 = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(valid_loader)\n",
    "        for X_batch, y_batch in pbar:\n",
    "            pbar.set_description(f'Epoch {epoch}/{EPOCHS} | loss : {valid_running_loss:.4f} | F1_score : {valid_running_epoch_f1:.4f}')\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).type(torch.cuda.LongTensor)\n",
    "            \n",
    "            y_pred = model.forward(X_batch)\n",
    "            _, preds = torch.max(y_pred, 1)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            valid_running_loss += loss.item() / len(y_batch)\n",
    "            valid_running_acc += torch.sum(preds == y_batch.data) / len(y_batch)\n",
    "            valid_running_epoch_f1 += f1_score(preds.cpu().numpy(), \n",
    "                                           y_batch.cpu().numpy(), average='macro')\n",
    "            \n",
    "    valid_epoch_loss = valid_running_loss / len(valid_loader)\n",
    "    valid_epoch_acc = valid_running_acc / len(valid_loader)\n",
    "    valid_epoch_f1 = valid_running_epoch_f1 / len(valid_loader)\n",
    "    \n",
    "    earlystop(valid_epoch_f1, model)\n",
    "    \n",
    "    print(f'Validation Loss : {valid_epoch_loss:.4f}, Accuracy : {valid_epoch_acc:.4f}, F1_score : {valid_epoch_f1:.4f}')   \n",
    "    \n",
    "    if valid_epoch_f1 > best_f1:\n",
    "        best_f1 = valid_epoch_f1\n",
    "    \n",
    "    val_acc_history.append(valid_epoch_acc)\n",
    "    val_loss_history.append(valid_epoch_loss)\n",
    "    \n",
    "    if earlystop.early_stop:\n",
    "        print('Early stopping')\n",
    "        model.load_state_dict(torch.load(f'{os.path.join(cfg.model_save_path, \"VIT\")}/checkpoint.pt'))\n",
    "        break\n",
    "\n",
    "print('Best Validation F1_score: {:.4f}'.format(best_f1))\n",
    "torch.save(model.state_dict(),\n",
    "           f'{os.path.join(cfg.model_save_path, \"VIT\")}/best_model_{best_f1:.4f}.pt')\n",
    "print('학습 종료!')\n",
    "end = time.time()\n",
    "print(f'학습 총 걸린 시간 : {end - start:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f95aac6-3887-4371-a7d6-bbc095585e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].plot(train_acc_history, label='train_acc')\n",
    "axes[0].plot(val_acc_history, label='val_acc')\n",
    "axes[1].plot(train_loss_history, label='train_loss')\n",
    "axes[1].plot(val_loss_history, label='val_loss')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ffc72fb-424d-4c3f-bcf7-51fb180170ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d4130-6e04-4654-88e0-1fde6ef38d29",
   "metadata": {},
   "source": [
    "## 6.1 Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00ad22e9-3616-4c87-9642-c90e8979bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_list, \n",
    "                         std_list)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acffdc4d-4bd5-4244-baa6-d4c56e66320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:37<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "best_model_path = [file for file in glob.glob(os.path.join(cfg.model_save_path, 'VIT/*')) \n",
    "                   if 'best_model' in file][0]\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(cfg.submission_dir, 'submission_{}.csv'.format(\n",
    "    datetime.datetime.today().astimezone(timezone(\"Asia/Seoul\")).strftime('%Y-%m-%d_%H:%M:%S'))), \n",
    "                  index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d433962-df60-4245-9ff5-ebebfe6abdda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
